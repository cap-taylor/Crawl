"""
ì‹¤ì‹œê°„ í´ë¦­ ë°©ì‹ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
- 13ë²ˆì§¸ ìƒí’ˆë¶€í„° ì‹œì‘
- 10ê°œ ìˆ˜ì§‘
- ëª¨ë“  DB ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶œë ¥
"""
import asyncio
import sys
import json
sys.path.append('/home/dino/MyProjects/Crawl')

from playwright.async_api import async_playwright
from src.utils.config import SELECTORS
from src.utils.selector_helper import SelectorHelper
import random
import re
from datetime import datetime

class RealClickCrawler:
    def __init__(self):
        self.products_data = []
        self.helper = SelectorHelper(debug=False)
        
    async def crawl(self):
        async with async_playwright() as p:
            browser = await p.firefox.launch(
                headless=False,
                slow_mo=1000
            )
            
            context = await browser.new_context(
                no_viewport=True,
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
                locale='ko-KR',
                timezone_id='Asia/Seoul',
                extra_http_headers={
                    'Accept-Language': 'ko-KR,ko;q=0.9'
                }
            )
            
            page = await context.new_page()
            
            # 1. ë„¤ì´ë²„ â†’ ì‡¼í•‘ â†’ ì—¬ì„±ì˜ë¥˜
            print("[1] ë„¤ì´ë²„ ë©”ì¸")
            await page.goto('https://www.naver.com')
            await asyncio.sleep(3)
            
            print("[2] ì‡¼í•‘ í´ë¦­")
            shopping = page.locator('#shortcutArea > ul > li:nth-child(4) > a')
            await shopping.click()
            await asyncio.sleep(3)
            
            page = context.pages[-1]
            await page.wait_for_load_state('networkidle')
            
            print("[3] ì¹´í…Œê³ ë¦¬ í´ë¦­")
            category_btn = await page.wait_for_selector('button:has-text("ì¹´í…Œê³ ë¦¬")')
            await category_btn.click()
            await asyncio.sleep(2)
            
            print("[4] ì—¬ì„±ì˜ë¥˜ í´ë¦­")
            womens = await page.wait_for_selector('a[data-name="ì—¬ì„±ì˜ë¥˜"]')
            await womens.click()
            
            # 20ì´ˆ ëŒ€ê¸° (ìº¡ì°¨ í•´ê²° ì‹œê°„)
            print("[ëŒ€ê¸°] 20ì´ˆ ëŒ€ê¸° ì¤‘ (ìº¡ì°¨ í’€ì–´ì£¼ì„¸ìš”)...")
            for i in range(20, 0, -5):
                print(f"  {i}ì´ˆ...")
                await asyncio.sleep(5)
            print("âœ… ëŒ€ê¸° ì™„ë£Œ!\n")

            # ì¶”ê°€ ë¡œë”© ëŒ€ê¸°
            print("[ë¡œë”©] ìƒí’ˆ ë¡œë”© ëŒ€ê¸° ì¤‘...")
            await asyncio.sleep(5)
            print("âœ… ë¡œë”© ì™„ë£Œ!\n")
            
            print("\n[5] ì‹¤ì‹œê°„ í´ë¦­ ìˆ˜ì§‘ ì‹œì‘\n")
            print("ëª©í‘œ: 13ë²ˆì§¸ë¶€í„° 10ê°œ ìˆ˜ì§‘ (1-12ë²ˆ ê´‘ê³  ìŠ¤í‚µ)\n")
            
            clicked_urls = set()  # í´ë¦­í•œ URLë§Œ ê¸°ë¡
            found_count = 0
            total_checked = 0  # ì „ì²´ í™•ì¸í•œ ìƒí’ˆ ìˆ˜
            skip_count = 12  # ì²˜ìŒ 12ê°œ ê´‘ê³  ìŠ¤í‚µ
            target_count = 10
            
            while found_count < target_count:
                # ìƒí’ˆ ë§í¬ë§Œ ì„ íƒ (íŒë§¤ì ë§í¬ ì œì™¸)
                # miniProductCard_link ë˜ëŠ” basicProductCard_link (mall ì œì™¸)
                products = await page.query_selector_all('a[class*="ProductCard_link"]')
                
                print(f"[ë””ë²„ê·¸] {len(products)}ê°œ ìƒí’ˆ ë§í¬ ë°œê²¬")
                
                if not products:
                    print("[ê²½ê³ ] ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    break
                
                new_products_found = False
                
                # ìƒí’ˆ ì²˜ë¦¬
                for idx, product in enumerate(products):
                    # ëª©í‘œ ë‹¬ì„± ì²´í¬
                    if found_count >= target_count:
                        break
                    
                    try:
                        href = await product.get_attribute('href')

                        # URL ê²€ì¦ (smartstore.naver.com/main/products/ìˆ«ì)
                        if not href or 'products' not in href:
                            continue

                        # ê´‘ê³  URL í•„í„°ë§
                        if 'ader.naver.com' in href:
                            continue
                        
                        # ì´ë¯¸ í´ë¦­í•œ ìƒí’ˆì€ ìŠ¤í‚µ
                        if href in clicked_urls:
                            continue
                        
                        # ìƒˆ ìƒí’ˆ ë°œê²¬
                        total_checked += 1
                        new_products_found = True
                        
                        # ê´‘ê³  ìŠ¤í‚µ (ì²˜ìŒ 12ê°œ)
                        if total_checked <= skip_count:
                            print(f"[{total_checked}ë²ˆì§¸] ê´‘ê³  ìŠ¤í‚µ")
                            clicked_urls.add(href)  # ìŠ¤í‚µí•œ ê²ƒë„ ê¸°ë¡
                            continue
                        
                        # í´ë¦­í•  ìƒí’ˆ
                        clicked_urls.add(href)
                        
                        # ìˆ˜ì§‘ ì‹œì‘
                        print(f"\n[{total_checked}ë²ˆì§¸ ìƒí’ˆ] í´ë¦­...", end="", flush=True)
                        
                        # ëœë¤ ëŒ€ê¸° (ì‚¬ëŒì²˜ëŸ¼)
                        await asyncio.sleep(random.uniform(2.0, 4.0))
                        
                        # ì§„ì§œ í´ë¦­ (ìƒˆ íƒ­ ìë™ ì—´ë¦¼)
                        await product.click()
                        await asyncio.sleep(2)
                        
                        # ìƒˆ íƒ­ ì°¾ê¸°
                        if len(context.pages) > 1:
                            new_tab = context.pages[-1]
                            # ë¬¸ì„œ ê¶Œì¥: networkidle 15ì´ˆ
                            try:
                                await new_tab.wait_for_load_state('networkidle', timeout=15000)
                            except:
                                await asyncio.sleep(5)
                            
                            # ë°ì´í„° ìˆ˜ì§‘
                            data = await self._collect_product_data(new_tab, href)
                            
                            if data:
                                self.products_data.append(data)
                                found_count += 1
                                
                                # ê°„ë‹¨ ì¶œë ¥
                                name = data.get('product_name', 'N/A')[:40]
                                tags = len(data.get('search_tags', []))
                                print(f" [{name}] - íƒœê·¸ {tags}ê°œ âœ…")
                            else:
                                print(" [SKIP] ìˆ˜ì§‘ ì‹¤íŒ¨")
                            
                            # íƒ­ ë‹«ê¸°
                            await new_tab.close()
                            await asyncio.sleep(random.uniform(1.0, 2.0))
                        
                    except Exception as e:
                        print(f" [ERROR] {str(e)[:50]}")
                        # í˜¹ì‹œ ì—´ë¦° íƒ­ ë‹«ê¸°
                        if len(context.pages) > 1:
                            await context.pages[-1].close()
                
                # ëª©í‘œ ë‹¬ì„± ì²´í¬
                if found_count >= target_count:
                    break
                
                # ìƒˆ ìƒí’ˆì´ ì—†ìœ¼ë©´ ìŠ¤í¬ë¡¤
                if not new_products_found:
                    print("\n[ìŠ¤í¬ë¡¤] ìƒˆ ìƒí’ˆ ë¡œë”©...")
                    await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                    await asyncio.sleep(3)
            
            print(f"\n{'='*60}")
            print(f"[ì™„ë£Œ] {found_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
            print(f"{'='*60}")
            
            await asyncio.sleep(3)
            await browser.close()
            
            return self.products_data
    
    async def _collect_product_data(self, page, url):
        """ìƒí’ˆ ë°ì´í„° ìˆ˜ì§‘ (DB ìŠ¤í‚¤ë§ˆ ì „ì²´)"""
        try:
            # ì—ëŸ¬ í˜ì´ì§€ ì²´í¬
            error_elem = await page.query_selector('text="í˜„ì¬ ì„œë¹„ìŠ¤ ì ‘ì†ì´ ë¶ˆê°€í•©ë‹ˆë‹¤"')
            if error_elem:
                return None
            
            # ìŠ¤í¬ë¡¤ (ê²€ìƒ‰íƒœê·¸ ìœ„ì¹˜)
            scroll_percent = random.uniform(0.40, 0.50)
            await page.evaluate(f'window.scrollTo(0, document.body.scrollHeight * {scroll_percent})')
            await asyncio.sleep(1)
            
            data = {}
            
            # 1. product_id (URLì—ì„œ ì¶”ì¶œ)
            match = re.search(r'/products/(\d+)', url)
            data['product_id'] = match.group(1) if match else None
            
            # 2. category_name
            data['category_name'] = "ì—¬ì„±ì˜ë¥˜"
            
            # 3. product_name
            elem = await self.helper.try_selectors(page, SELECTORS['product_name'], "ìƒí’ˆëª…")
            data['product_name'] = await self.helper.extract_text(elem, "ìƒí’ˆëª…")
            
            # 4. brand_name
            elem = await self.helper.try_selectors(page, SELECTORS['brand_name'], "ë¸Œëœë“œ")
            data['brand_name'] = await self.helper.extract_text(elem, "ë¸Œëœë“œ")
            
            # 5. price
            elem = await self.helper.try_selectors(page, SELECTORS['price'], "ê°€ê²©")
            price_text = await self.helper.extract_text(elem, "ê°€ê²©")
            data['price'] = self.helper.clean_price(price_text)
            
            # 6. discount_rate
            elem = await self.helper.try_selectors(page, SELECTORS['discount_rate'], "í• ì¸ìœ¨")
            discount_text = await self.helper.extract_text(elem, "í• ì¸ìœ¨")
            data['discount_rate'] = self.helper.clean_discount_rate(discount_text)
            
            # 7. review_count
            elem = await self.helper.try_selectors(page, SELECTORS['review_count'], "ë¦¬ë·° ìˆ˜")
            review_text = await self.helper.extract_text(elem, "ë¦¬ë·° ìˆ˜")
            data['review_count'] = self.helper.clean_review_count(review_text)
            
            # 8. rating
            elem = await self.helper.try_selectors(page, SELECTORS['rating'], "í‰ì ")
            rating_text = await self.helper.extract_text(elem, "í‰ì ")
            data['rating'] = self.helper.clean_rating(rating_text)
            
            # 9. search_tags
            tags = []
            try:
                all_links = await page.query_selector_all('a')
                for link in all_links[:100]:
                    text = await self.helper.extract_text(link)
                    if text and text.strip().startswith('#'):
                        clean_tag = text.strip().replace('#', '').strip()
                        if 1 < len(clean_tag) < 30 and clean_tag not in tags:
                            tags.append(clean_tag)
            except:
                pass
            data['search_tags'] = tags
            
            # 10. product_url
            data['product_url'] = url
            
            # 11. thumbnail_url
            elem = await self.helper.try_selectors(page, SELECTORS['thumbnail'], "ì¸ë„¤ì¼")
            data['thumbnail_url'] = await self.helper.extract_attribute(elem, "src", "ì¸ë„¤ì¼")
            
            # 12. is_sold_out
            elem = await self.helper.try_selectors(page, SELECTORS['is_sold_out'], "í’ˆì ˆ")
            data['is_sold_out'] = (elem is not None)
            
            # 13. crawled_at
            data['crawled_at'] = datetime.now().isoformat()
            
            # 14. updated_at
            data['updated_at'] = datetime.now().isoformat()
            
            # ê²€ì¦: ìƒí’ˆëª… í•„ìˆ˜
            if not data['product_name'] or len(data['product_name']) < 5:
                return None
            
            return data
            
        except Exception as e:
            print(f"[ìˆ˜ì§‘ ì˜¤ë¥˜] {str(e)[:50]}")
            return None

async def main():
    crawler = RealClickCrawler()
    products = await crawler.crawl()
    
    if products:
        print(f"\n{'='*80}")
        print("ğŸ“¦ ìˆ˜ì§‘ëœ ë°ì´í„° (DB ìŠ¤í‚¤ë§ˆ ì „ì²´)")
        print(f"{'='*80}\n")
        
        for idx, product in enumerate(products, 1):
            print(f"[ìƒí’ˆ {idx}]")
            print(json.dumps(product, ensure_ascii=False, indent=2))
            print("-" * 80)
    
    print(f"\nì´ {len(products)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main())
