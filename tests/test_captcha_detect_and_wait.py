"""
ìº¡ì°¨ ê°ì§€ ë° ìˆ˜ë™ í•´ê²° ëŒ€ê¸°
ìº¡ì°¨ê°€ ë‚˜íƒ€ë‚˜ë©´ ì•Œë¦¼ì„ ì£¼ê³  ì‚¬ìš©ìê°€ í•´ê²°í•  ë•Œê¹Œì§€ ëŒ€ê¸°
"""
import asyncio
import json
from datetime import datetime
from playwright.async_api import async_playwright
import re

class CaptchaDetectorAndWait:
    def __init__(self, headless=False):
        self.headless = headless
        self.product_data = {}

    async def collect_product_info(self, detail_page, product_id):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘"""
        product_info = {'product_id': product_id}

        try:
            # í˜ì´ì§€ í•˜ë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤ (í•´ì‹œíƒœê·¸ëŠ” í•˜ë‹¨ì— ìˆìŒ)
            await detail_page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await asyncio.sleep(2)

            # ìƒí’ˆëª… (ìŠ¤í¬ë¦°ìƒ·ì—ì„œ í™•ì¸: h3 íƒœê·¸)
            # ë””ë²„ê¹…: ëª¨ë“  h3 ì°¾ê¸°
            all_h3 = await detail_page.query_selector_all('h3')
            print(f"   [ë””ë²„ê·¸] ì°¾ì€ h3 íƒœê·¸ ê°œìˆ˜: {len(all_h3)}")
            for idx, h3 in enumerate(all_h3[:5]):
                text = await h3.inner_text()
                print(f"   [ë””ë²„ê·¸] h3[{idx}]: {text[:50] if text else '(ë¹ˆ ë¬¸ìì—´)'}...")

            name_elem = await detail_page.query_selector('h3')
            if not name_elem:
                name_elem = await detail_page.query_selector('h2')
            if not name_elem:
                name_elem = await detail_page.query_selector('h1')
            if name_elem:
                name_text = (await name_elem.inner_text()).strip()
                if name_text:
                    product_info['product_name'] = name_text
                    print(f"   [ë””ë²„ê·¸] ìƒí’ˆëª… ìˆ˜ì§‘ ì„±ê³µ: {name_text[:30]}...")

            # ê°€ê²© (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
            price_selectors = [
                'strong.zPdReA',  # ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ê²©
                'span[class*="price"]',
                'strong[class*="price"]',
                'div[class*="price"] strong',
                'em[class*="price"]'
            ]
            for price_sel in price_selectors:
                price_elem = await detail_page.query_selector(price_sel)
                if price_elem:
                    price_text = await price_elem.inner_text()
                    price_nums = re.findall(r'[\d,]+', price_text)
                    if price_nums:
                        product_info['price'] = price_nums[0].replace(',', '')
                        break

            # í•´ì‹œíƒœê·¸ ì°¾ê¸° (ìŠ¤í¬ë¦°ìƒ·ì—ì„œ í™•ì¸: a._1JswZdbu)
            hashtags = []
            # ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„ (í•˜ë“œì½”ë”© ì—†ì´)
            tag_selectors = [
                'a[class*="_1JswZdbu"]',  # ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ìŠ¤íƒ€ì¼
                'a[href*="/search"][class*="linkAnchor"]',  # ê²€ìƒ‰ ë§í¬
                'a.oe34bePu',  # ë‹¤ë¥¸ ìŠ¤íƒ€ì¼
                'ul[class*="related"] a',  # ì—°ê´€ íƒœê·¸
                'div[class*="tag"] a',  # íƒœê·¸ ì˜ì—­
                'a[href*="/search/"]'  # ì¼ë°˜ ê²€ìƒ‰ ë§í¬
            ]

            for tag_sel in tag_selectors:
                tag_elements = await detail_page.query_selector_all(tag_sel)
                if tag_elements:
                    print(f"   [ë””ë²„ê·¸] ê²€ìƒ‰íƒœê·¸ ë°œê²¬: {tag_sel} ({len(tag_elements)}ê°œ)")
                    break

            for tag_elem in tag_elements[:30]:
                tag_text = await tag_elem.inner_text()
                if tag_text and tag_text.strip():
                    hashtags.append(tag_text.strip())

            if hashtags:
                product_info['search_tags'] = ', '.join(hashtags)
                print(f"   [ë””ë²„ê·¸] ê²€ìƒ‰íƒœê·¸ ìˆ˜ì§‘: {len(hashtags)}ê°œ")

            # ë¦¬ë·°
            review_elem = await detail_page.query_selector('[class*="review"]')
            if review_elem:
                review_text = await review_elem.inner_text()
                numbers = re.findall(r'[\d,]+', review_text)
                if numbers:
                    product_info['review_count'] = numbers[0].replace(',', '')

            # í‰ì 
            rating_elem = await detail_page.query_selector('[class*="star"], [class*="rating"]')
            if rating_elem:
                rating_text = await rating_elem.inner_text()
                rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                if rating_match:
                    product_info['rating'] = rating_match.group(1)

            # URL
            product_info['url'] = detail_page.url

            return product_info

        except Exception as e:
            print(f"   ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)[:50]}")
            return None

    async def crawl_with_manual_captcha(self):
        """ìº¡ì°¨ ê°ì§€ ë° ìˆ˜ë™ í•´ê²° ëŒ€ê¸° í¬ë¡¤ë§"""
        async with async_playwright() as p:
            try:
                print("[ì‹œì‘] Firefox ë¸Œë¼ìš°ì € ì‹¤í–‰...")
                browser = await p.firefox.launch(
                    headless=False,  # í•­ìƒ ë³´ì´ë„ë¡
                    slow_mo=500  # CRAWLING_LESSONS_LEARNED.mdì— ë”°ë¼ ì²œì²œíˆ
                )

                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
                    locale='ko-KR',
                    timezone_id='Asia/Seoul'
                )

                page = await context.new_page()

                # 1. ë„¤ì´ë²„ ë©”ì¸ ì ‘ì†
                print("\n[1ë‹¨ê³„] ë„¤ì´ë²„ ë©”ì¸ ì ‘ì†...")
                await page.goto('https://www.naver.com', timeout=60000)
                await page.wait_for_load_state('domcontentloaded')
                await asyncio.sleep(1)  # ë” ì§§ê²Œ

                # ë„¤ì´ë²„ ë©”ì¸ì—ì„œëŠ” ìº¡ì°¨ ì²´í¬ ìƒëµ (ìº¡ì°¨ ì—†ìŒ)

                # 2. ì‡¼í•‘ í´ë¦­
                print("\n[2ë‹¨ê³„] ì‡¼í•‘ í˜ì´ì§€ë¡œ ì´ë™...")
                shopping_link = await page.wait_for_selector('#shortcutArea > ul > li:nth-child(4) > a')
                await shopping_link.click()
                await asyncio.sleep(1.5)  # ë” ë¹ ë¥´ê²Œ

                # ìƒˆ íƒ­ ì „í™˜ (CRAWLING_LESSONS_LEARNED.md: ìƒˆ íƒ­ìœ¼ë¡œ ì—´ì–´ì•¼ ìº¡ì°¨ ì•ˆ ë‚˜ì˜´!)
                await asyncio.sleep(3)  # íƒ­ ì—´ë¦¬ê¸° ëŒ€ê¸°
                all_pages = context.pages
                if len(all_pages) > 1:
                    page = all_pages[-1]  # ë§ˆì§€ë§‰ íƒ­ = ì‡¼í•‘ íƒ­
                    await page.wait_for_load_state('networkidle')
                    print("âœ… ìƒˆ íƒ­(ì‡¼í•‘)ìœ¼ë¡œ ì „í™˜ ì™„ë£Œ")

                # ì‡¼í•‘ ë©”ì¸ì—ì„œë„ ìº¡ì°¨ ì²´í¬ ìƒëµ

                # 3. ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ í´ë¦­
                print("\n[3ë‹¨ê³„] ì¹´í…Œê³ ë¦¬ ë©”ë‰´ ì—´ê¸°...")
                category_btn = await page.wait_for_selector('button:has-text("ì¹´í…Œê³ ë¦¬")')
                await category_btn.click()
                await asyncio.sleep(1)  # ë” ë¹ ë¥´ê²Œ

                # 4. ì—¬ì„±ì˜ë¥˜ í´ë¦­ (data-name ì†ì„± ì‚¬ìš©ì´ ë” ì•ˆì •ì )
                print("\n[4ë‹¨ê³„] ì—¬ì„±ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì„ íƒ...")
                # ë¨¼ì € í˜¸ë²„í•˜ê³  í´ë¦­ (ì‚¬ëŒì²˜ëŸ¼)
                womens = await page.wait_for_selector('a[data-name="ì—¬ì„±ì˜ë¥˜"]')
                await womens.hover()  # í˜¸ë²„ ì¶”ê°€
                await asyncio.sleep(1)  # í˜¸ë²„ í›„ ëŒ€ê¸°
                await womens.click()

                print("\nâ³ ìº¡ì°¨ê°€ ëœ° ìˆ˜ ìˆìŠµë‹ˆë‹¤. 30ì´ˆ ëŒ€ê¸° ì¤‘...")
                print("ìº¡ì°¨ê°€ ë‚˜íƒ€ë‚˜ë©´ ì§ì ‘ í•´ê²°í•´ì£¼ì„¸ìš”!")
                print("í•œê¸€ ì…ë ¥ ì•ˆë˜ë©´: ë©”ëª¨ì¥ì— ë‹µ ì ê³  ë³µì‚¬-ë¶™ì—¬ë„£ê¸° í•˜ì„¸ìš”!")
                await asyncio.sleep(30)  # 30ì´ˆ ëŒ€ê¸° - ìº¡ì°¨ í•´ê²° ì‹œê°„

                await page.wait_for_load_state('networkidle')
                print("âœ… ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")

                # 5. ì¼ë°˜ ìƒí’ˆ ì²« ë²ˆì§¸ í´ë¦­í•´ì„œ ì •ë³´ ìˆ˜ì§‘
                print("\n[5ë‹¨ê³„] ì¼ë°˜ ìƒí’ˆ ì²« ë²ˆì§¸ í´ë¦­...")

                # í˜ì´ì§€ ë¡œë”© í™•ì¸ì„ ìœ„í•œ ì¶”ê°€ ëŒ€ê¸°
                await asyncio.sleep(3)

                # ì¼ë°˜ ìƒí’ˆ ì˜ì—­ê¹Œì§€ ìŠ¤í¬ë¡¤
                print("ì¼ë°˜ ìƒí’ˆ ì˜ì—­ê¹Œì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
                for i in range(3):  # ì´ˆê¸° ìŠ¤í¬ë¡¤
                    await page.evaluate(f'window.scrollBy(0, 1000)')
                    await asyncio.sleep(1)

                # ì¼ë°˜ ìƒí’ˆ ì²« ë²ˆì§¸ ì°¾ê¸° (docs/selectors/ì¼ë°˜ ìƒí’ˆ.txt ì°¸ê³ )
                first_product = await page.wait_for_selector('#composite-card-list > div > ul:nth-child(1) > li:nth-child(1) > div > a')

                if first_product:
                    print("âœ… ì¼ë°˜ ìƒí’ˆ ì²« ë²ˆì§¸ ì°¾ìŒ!")

                    # ìƒí’ˆ ID ì¶”ì¶œ
                    href = await first_product.get_attribute('href')
                    id_match = re.search(r'/products/(\d+)', href)
                    product_id = id_match.group(1) if id_match else 'unknown'
                    print(f"ìƒí’ˆ ID: {product_id}")

                    # ìƒí’ˆ í´ë¦­
                    await first_product.click()
                    await asyncio.sleep(3)

                    # ìƒˆ íƒ­ìœ¼ë¡œ ì „í™˜
                    all_pages = context.pages
                    if len(all_pages) > 2:
                        detail_page = all_pages[-1]
                        await detail_page.wait_for_load_state('networkidle')
                        print("âœ… ìƒì„¸ í˜ì´ì§€ ì—´ë¦¼")

                        # ë””ë²„ê¹…: ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                        await detail_page.screenshot(path='data/product_detail_debug.png')

                        # ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
                        product_info = await self.collect_product_info(detail_page, product_id)

                        if product_info:
                            print(f"\n{'='*60}")
                            print("ğŸ“¦ ìˆ˜ì§‘ëœ ìƒí’ˆ ì •ë³´:")
                            print(f"{'='*60}")
                            print(f"ğŸ†” ìƒí’ˆ ID: {product_info.get('product_id', 'N/A')}")
                            print(f"ğŸ“¦ ìƒí’ˆëª…: {product_info.get('product_name', 'N/A')}")
                            print(f"ğŸ’° ê°€ê²©: {product_info.get('price', 'N/A')}ì›")
                            print(f"â­ í‰ì : {product_info.get('rating', 'N/A')}")
                            print(f"ğŸ’¬ ë¦¬ë·° ìˆ˜: {product_info.get('review_count', 'N/A')}")
                            print(f"ğŸ”— URL: {product_info.get('url', 'N/A')}")
                            print(f"\nğŸ·ï¸ ê²€ìƒ‰íƒœê·¸ (í•´ì‹œíƒœê·¸):")
                            if product_info.get('search_tags'):
                                print(f"{product_info['search_tags']}")
                            else:
                                print("   (ê²€ìƒ‰íƒœê·¸ ì—†ìŒ)")
                            print(f"{'='*60}")

                            # JSON ì €ì¥
                            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                            json_file = f'data/product_{timestamp}.json'
                            with open(json_file, 'w', encoding='utf-8') as f:
                                json.dump({
                                    'category': 'ì—¬ì„±ì˜ë¥˜',
                                    'category_id': 10000107,
                                    'crawled_at': datetime.now().isoformat(),
                                    'product': product_info
                                }, f, ensure_ascii=False, indent=2)
                            print(f"\nğŸ’¾ JSON ì €ì¥: {json_file}")

                            # DB ì €ì¥ (í˜„ì¬ëŠ” JSONë§Œ)
                            print("\nğŸ“ DB ì €ì¥ìš© ë°ì´í„° (PostgreSQL ì¤€ë¹„ í›„ ì €ì¥ ê°€ëŠ¥):")
                            self.product_data = product_info

                        await detail_page.close()
                else:
                    print("âŒ ì¼ë°˜ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    await page.screenshot(path='data/no_products.png')

                await browser.close()
                return self.product_data

            except Exception as e:
                print(f"\n[ì˜¤ë¥˜] {str(e)}")
                import traceback
                traceback.print_exc()
                return None


if __name__ == "__main__":
    async def main():
        print("="*60)
        print("ë„¤ì´ë²„ ì‡¼í•‘ í¬ë¡¤ëŸ¬ - ìº¡ì°¨ ê°ì§€ ë° ìˆ˜ë™ í•´ê²°")
        print("="*60)
        print("ìº¡ì°¨ê°€ ë‚˜íƒ€ë‚˜ë©´ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ í•´ê²°í•´ì£¼ì„¸ìš”.")
        print("="*60)

        detector = CaptchaDetectorAndWait(headless=False)
        data = await detector.crawl_with_manual_captcha()

        if data:
            print("\nğŸ‰ í¬ë¡¤ë§ ì„±ê³µ!")
            print(f"ìˆ˜ì§‘ëœ ìƒí’ˆ: {data.get('product_name', 'N/A')}")
        else:
            print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨")

    asyncio.run(main())