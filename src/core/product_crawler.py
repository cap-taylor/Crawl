"""
ì—¬ì„±ì˜ë¥˜ í¬ë¡¤ë§ - ìˆ˜ë™ ìº¡ì°¨ í•´ê²° ë²„ì „
ìº¡ì°¨ê°€ ë‚˜íƒ€ë‚˜ë©´ ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ í•´ê²° í›„ ê³„ì† ì§„í–‰
ë¦¬íŒ©í† ë§: êµ¬ì¡° ê¸°ë°˜ ì…€ë ‰í„° + ë‹¤ì¤‘ Fallback ì‹œìŠ¤í…œ
"""
import asyncio
import json
import sys
from datetime import datetime
from typing import Optional
from playwright.async_api import async_playwright
import re

# DB ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append('/home/dino/MyProjects/Crawl')
from src.database.db_connector import save_to_database
from src.utils.config import SELECTORS
from src.utils.selector_helper import SelectorHelper

class WomensClothingManualCaptcha:
    def __init__(self, headless=False, product_count: Optional[int] = 1, enable_screenshot=False,
                 category_name: str = "ì—¬ì„±ì˜ë¥˜", category_id: str = "10000107", debug_selectors: bool = False,
                 specific_index: Optional[int] = None):  # íŠ¹ì • ì¸ë±ìŠ¤ë§Œ ìˆ˜ì§‘
        self.headless = headless
        self.product_count = product_count  # ìˆ˜ì§‘í•  ìƒí’ˆ ê°œìˆ˜ (Noneì´ë©´ ë¬´í•œ)
        self.enable_screenshot = enable_screenshot  # ìŠ¤í¬ë¦°ìƒ· í™œì„±í™” ì—¬ë¶€
        self.category_name = category_name  # ì¹´í…Œê³ ë¦¬ ì´ë¦„
        self.category_id = category_id  # ì¹´í…Œê³ ë¦¬ ID
        self.products_data = []  # ì—¬ëŸ¬ ìƒí’ˆ ì €ì¥
        self.should_stop = False  # ì¤‘ì§€ í”Œë˜ê·¸ (GUIì—ì„œ ì„¤ì •)
        self.helper = SelectorHelper(debug=debug_selectors)  # ì…€ë ‰í„° Helper
        self.specific_index = specific_index  # íŠ¹ì • ì¸ë±ìŠ¤ë§Œ ìˆ˜ì§‘ (0-based)

    async def wait_for_captcha_solve(self, page):
        """ìº¡ì°¨ í•´ê²° ëŒ€ê¸° - 15ì´ˆ ê³ ì •"""
        print("\n" + "="*60)
        print("âš ï¸  ìº¡ì°¨ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("="*60)
        print("ë¸Œë¼ìš°ì €ì—ì„œ ìº¡ì°¨ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í•´ê²°í•´ì£¼ì„¸ìš”:")
        print("1. ìº¡ì°¨ ì´ë¯¸ì§€ì— í‘œì‹œëœ ë¬¸ìë¥¼ ì…ë ¥")
        print("2. 'í™•ì¸' ë²„íŠ¼ í´ë¦­")
        print("3. ì •ìƒ í˜ì´ì§€ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°")
        print("="*60)
        print("â° 15ì´ˆ ë™ì•ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")

        # 15ì´ˆ ëŒ€ê¸° (ê³ ì •)
        for i in range(15, 0, -5):
            print(f"[ëŒ€ê¸°] ë‚¨ì€ ì‹œê°„: {i}ì´ˆ...")
            await asyncio.sleep(5)

        print("âœ… ëŒ€ê¸° ì™„ë£Œ! í¬ë¡¤ë§ì„ ê³„ì†í•©ë‹ˆë‹¤...")
        await asyncio.sleep(2)

    async def crawl_with_manual_captcha(self):
        """ìˆ˜ë™ ìº¡ì°¨ í•´ê²° ë°©ì‹ìœ¼ë¡œ í¬ë¡¤ë§"""
        async with async_playwright() as p:
            try:
                print("[ì‹œì‘] Firefox ë¸Œë¼ìš°ì € ì‹¤í–‰ (ì „ì²´í™”ë©´)...")
                browser = await p.firefox.launch(
                    headless=False,  # í•­ìƒ ë³´ì´ë„ë¡
                    slow_mo=500
                    # FirefoxëŠ” --start-maximized ì§€ì› ì•ˆí•¨
                )

                context = await browser.new_context(
                    no_viewport=True,  # ì „ì²´í™”ë©´
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
                    locale='ko-KR',
                    timezone_id='Asia/Seoul'
                )

                page = await context.new_page()

                # 1. ë„¤ì´ë²„ ë©”ì¸ ì ‘ì†
                print("[ì ‘ì†] ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€...")
                await page.goto('https://www.naver.com')
                await page.wait_for_load_state('domcontentloaded')
                await asyncio.sleep(3)

                # 2. ì‡¼í•‘ í´ë¦­ (ë¬¸ì„œì—ì„œ ê²€ì¦ëœ ë°©ë²• ì‚¬ìš©)
                print("[í´ë¦­] ì‡¼í•‘ ì•„ì´ì½˜...")

                # í˜ì´ì§€ ìƒë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤ (ì‡¼í•‘ ë²„íŠ¼ì´ í•­ìƒ ìƒë‹¨ì— ìˆìŒ)
                await page.evaluate('window.scrollTo(0, 0)')
                await asyncio.sleep(1)

                # ë¬¸ì„œì—ì„œ ê²€ì¦ëœ ì…€ë ‰í„° ì‚¬ìš©
                shopping_selector = '#shortcutArea > ul > li:nth-child(4) > a'
                try:
                    shopping_link = page.locator(shopping_selector)
                    await shopping_link.click(timeout=10000)
                except Exception as e:
                    print(f"[ì˜¤ë¥˜] ì‡¼í•‘ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {str(e)[:100]}")
                    await browser.close()
                    return None

                await asyncio.sleep(3)

                # ìƒˆ íƒ­ ì „í™˜
                all_pages = context.pages
                if len(all_pages) > 1:
                    page = all_pages[-1]
                    await page.wait_for_load_state('networkidle')

                # ìº¡ì°¨ ì²´í¬
                await asyncio.sleep(2)
                if await page.query_selector('text="ë³´ì•ˆ í™•ì¸ì„ ì™„ë£Œí•´ ì£¼ì„¸ìš”"'):
                    await self.wait_for_captcha_solve(page)

                # 3. ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ í´ë¦­
                print("[í´ë¦­] ì¹´í…Œê³ ë¦¬ ë©”ë‰´ ì—´ê¸°...")
                category_btn = await page.wait_for_selector('button:has-text("ì¹´í…Œê³ ë¦¬")')
                await category_btn.click()
                await asyncio.sleep(2)

                # 4. ì—¬ì„±ì˜ë¥˜ í´ë¦­
                print(f"[í´ë¦­] {self.category_name} ì¹´í…Œê³ ë¦¬...")
                womens = await page.wait_for_selector(f'a[data-name="{self.category_name}"]')
                await womens.click()

                # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° (ì¶©ë¶„í•œ ì‹œê°„)
                print("[ëŒ€ê¸°] í˜ì´ì§€ ë¡œë”© ì¤‘...")
                await asyncio.sleep(5)

                # ìº¡ì°¨ ì²´í¬
                print("[í™•ì¸] ìº¡ì°¨ ì—¬ë¶€ ì²´í¬ ì¤‘...")
                captcha_elem = await self.helper.try_selectors(
                    page, SELECTORS['captcha_indicators'], "ìº¡ì°¨"
                )
                captcha_detected = captcha_elem is not None

                if captcha_detected:
                    await self.wait_for_captcha_solve(page)
                else:
                    print("[í™•ì¸] ìº¡ì°¨ ì—†ìŒ - ì •ìƒ ì§„í–‰")

                # ìµœì¢… í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                try:
                    await page.wait_for_load_state('networkidle', timeout=10000)
                except:
                    pass
                await asyncio.sleep(2)

                # 5. ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
                if self.specific_index is not None:
                    print(f"\n[íƒìƒ‰] {self.specific_index + 1}ë²ˆì§¸ ìƒí’ˆë§Œ ìˆ˜ì§‘...")
                elif self.product_count is None:
                    print(f"\n[íƒìƒ‰] ë¬´í•œ ëª¨ë“œ - ëª¨ë“  ìƒí’ˆ ìˆ˜ì§‘ ì‹œì‘...")
                else:
                    print(f"\n[íƒìƒ‰] ìƒí’ˆ {self.product_count}ê°œ ìˆ˜ì§‘ ì‹œì‘...")

                # ìƒí’ˆ ë§í¬ ì°¾ê¸° (ì´ë¯¸ì§€ê°€ ìˆëŠ” ë§í¬ë§Œ = ìƒí’ˆ ì¸ë„¤ì¼ ë§í¬)
                print("[íƒìƒ‰] ìƒí’ˆ ì´ë¯¸ì§€ ë§í¬ ì°¾ëŠ” ì¤‘...")
                product_elements = await page.query_selector_all('a[href*="/products/"]:has(img)')

                if not product_elements:
                    # Fallback: ê¸°ì¡´ ë°©ì‹
                    print("[Fallback] ê¸°ì¡´ ì…€ë ‰í„°ë¡œ ì¬ì‹œë„...")
                    product_elements = await self.helper.try_selectors(
                        page, SELECTORS['product_links'], "ìƒí’ˆ ë§í¬", multiple=True
                    )

                # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
                all_product_elements = []
                if product_elements:
                    seen_urls = set()
                    for elem in product_elements:
                        href = await elem.get_attribute('href')
                        # ì •í™•í•œ ìƒí’ˆ URL íŒ¨í„´ í™•ì¸: /products/ìˆ«ì
                        if href and '/products/' in href and re.search(r'/products/\d+', href) and href not in seen_urls:
                            all_product_elements.append(elem)
                            seen_urls.add(href)
                    print(f"[ë°œê²¬] ì´ {len(all_product_elements)}ê°œ ìƒí’ˆ ë°œê²¬")
                else:
                    all_product_elements = []

                if not all_product_elements:
                    print("[ê²½ê³ ] ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    await page.screenshot(path='data/final_page.png')
                    print("[ìŠ¤í¬ë¦°ìƒ·] data/final_page.png ì €ì¥ë¨")
                else:
                    # specific_indexê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ìƒí’ˆë§Œ ìˆ˜ì§‘
                    if self.specific_index is not None:
                        print(f"[ì‹œì‘] {self.specific_index + 1}ë²ˆì§¸ ìƒí’ˆë¶€í„° ìˆ˜ì§‘ ì‹œì‘...\n")
                        idx = self.specific_index  # íŠ¹ì • ì¸ë±ìŠ¤ë¡œ ì‹œì‘
                        found_count = 0
                        max_attempts = 10  # ìµœëŒ€ 10ê°œê¹Œì§€ ì‹œë„

                        # ì„±ê³µí•  ë•Œê¹Œì§€ ë‹¤ìŒ ìƒí’ˆ ì‹œë„ (ìµœëŒ€ max_attemptsê°œ)
                        while found_count == 0 and idx < len(all_product_elements) and (idx - self.specific_index) < max_attempts:
                            print(f"\n[{idx+1}ë²ˆì§¸ ìƒí’ˆ] ìˆ˜ì§‘ ì‹œë„...")

                            # specific_index ëª¨ë“œ: ì²˜ìŒ ì°¾ì€ element ê·¸ëŒ€ë¡œ ì‚¬ìš©
                            product_elem = all_product_elements[idx]
                            href = await product_elem.get_attribute('href')

                            if not href:
                                print(f"#{idx+1} [SKIP] URLì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ - ë‹¤ìŒ ìƒí’ˆ ì‹œë„")
                                idx += 1
                                continue

                            try:
                                    # ìƒí’ˆ í´ë¦­ (viewportë¡œ ìŠ¤í¬ë¡¤ í›„ í´ë¦­)
                                    try:
                                        # 1. Elementë¥¼ í™”ë©´ì— ë³´ì´ë„ë¡ ìŠ¤í¬ë¡¤
                                        await product_elem.scroll_into_view_if_needed()
                                        await asyncio.sleep(0.5)

                                        # 2. í´ë¦­ ì‹œë„ (íƒ€ì„ì•„ì›ƒ 10ì´ˆ)
                                        await product_elem.click(timeout=10000)
                                    except Exception as click_error:
                                        # 3. ì‹¤íŒ¨ ì‹œ ê°•ì œ í´ë¦­ ì‹œë„
                                        print(f"   [ì¬ì‹œë„] ê°•ì œ í´ë¦­ ì‹œë„...")
                                        await product_elem.click(force=True, timeout=5000)

                                    await asyncio.sleep(3)

                                    # ìƒˆ íƒ­ ì°¾ê¸°
                                    all_pages = context.pages
                                    if len(all_pages) <= 1:
                                        print(f"#{idx+1} [SKIP] íƒ­ ì—´ë¦¼ ì‹¤íŒ¨ - ë‹¤ìŒ ìƒí’ˆ ì‹œë„")
                                        idx += 1
                                        continue

                                    detail_page = all_pages[-1]
                                    await detail_page.wait_for_load_state('domcontentloaded')
                                    await asyncio.sleep(1)

                                    # URL ê²€ì¦: ìƒí’ˆ í˜ì´ì§€ì¸ì§€ í™•ì¸
                                    current_url = detail_page.url
                                    if not re.search(r'/products/\d+', current_url):
                                        print(f"#{idx+1} [SKIP] ì˜ëª»ëœ í˜ì´ì§€ (ìŠ¤í† ì–´ í˜ì´ì§€?) - ë‹¤ìŒ ìƒí’ˆ ì‹œë„")
                                        print(f"         URL: {current_url[:70]}")
                                        await detail_page.close()
                                        idx += 1
                                        continue

                                    # ì •ìƒ ìƒí’ˆ í˜ì´ì§€ - ìˆ˜ì§‘ ì§„í–‰
                                    try:
                                        await detail_page.wait_for_load_state('networkidle', timeout=10000)
                                    except:
                                        pass
                                    await asyncio.sleep(1)

                                    # ìŠ¤í¬ë¡¤ (ê²€ìƒ‰íƒœê·¸ ìœ„ì¹˜)
                                    await detail_page.evaluate('window.scrollTo(0, document.body.scrollHeight * 0.4)')
                                    await asyncio.sleep(2)

                                    # ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
                                    self.product_data = {}
                                    self.product_data['product_url'] = href
                                    await self._collect_detail_page_info(detail_page)

                                    # í•œ ì¤„ ìš”ì•½ ì¶œë ¥
                                    detail_info = self.product_data.get('detail_page_info', {})
                                    product_name = detail_info.get('detail_product_name', 'N/A')

                                    # ìƒí’ˆëª… ê²€ì¦ (ì˜ëª»ëœ í˜ì´ì§€ì¸ì§€ í™•ì¸)
                                    invalid_keywords = ['ë³¸ë¬¸', 'ë°”ë¡œê°€ê¸°', 'ë„¤ì´ë²„', 'ë¡œê·¸ì¸', 'ì„œë¹„ìŠ¤', 'ìŠ¤í† ì–´ í™ˆ', 'For w', 'NAVER']
                                    is_invalid = (
                                        not product_name or
                                        product_name == 'N/A' or
                                        len(product_name) < 5 or  # ë„ˆë¬´ ì§§ìŒ
                                        any(keyword in product_name for keyword in invalid_keywords)  # ë©”ë‰´ í…ìŠ¤íŠ¸
                                    )

                                    if is_invalid:
                                        print(f"#{idx+1} [SKIP] ì˜ëª»ëœ ìƒí’ˆëª…: '{product_name[:30]}' - ë‹¤ìŒ ìƒí’ˆ ì‹œë„")
                                        print(f"         í˜„ì¬ URL: {detail_page.url[:70]}")
                                        await detail_page.close()
                                        idx += 1
                                        continue

                                    tags_count = len(detail_info.get('search_tags', []))

                                    # ìˆ˜ì§‘ ì™„ë£Œ
                                    self.products_data.append(self.product_data.copy())
                                    found_count += 1

                                    print(f"#{idx+1} [{product_name[:30]}] - íƒœê·¸ {tags_count}ê°œ âœ… ìˆ˜ì§‘ ì„±ê³µ!")

                                    # íƒ­ ë‹«ê¸°
                                    await detail_page.close()
                                    await asyncio.sleep(1)

                            except Exception as e:
                                print(f"#{idx+1} [ERROR] {str(e)[:50]} - ë‹¤ìŒ ìƒí’ˆ ì‹œë„")
                                try:
                                    if len(context.pages) > 2:
                                        await context.pages[-1].close()
                                except:
                                    pass
                                idx += 1
                                continue

                        if found_count == 0:
                            print(f"\n[ê²½ê³ ] {self.specific_index + 1}ë²ˆì§¸ë¶€í„° {max_attempts}ê°œ ì‹œë„í–ˆì§€ë§Œ ìˆ˜ì§‘ ì‹¤íŒ¨")

                        print(f"\n{'='*60}")
                        print(f"[ì™„ë£Œ] {idx+1}ë²ˆì§¸ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ!")
                        print(f"{'='*60}")
                    else:
                        # ëª¨ë“  ìƒí’ˆ ìˆ˜ì§‘ (ê´‘ê³  í¬í•¨, ê²€ìƒ‰íƒœê·¸ ì—†ì–´ë„ ìˆ˜ì§‘)
                        print(f"[ì‹œì‘] 1ë²ˆì§¸ ìƒí’ˆë¶€í„° ëª¨ë“  ìƒí’ˆ ìˆ˜ì§‘...\n")
                        print(f"[ì •ë³´] ê´‘ê³  í¬í•¨, ê²€ìƒ‰íƒœê·¸ ì—†ì–´ë„ ìˆ˜ì§‘\n")

                        found_count = 0
                        idx = 0  # 1ë²ˆì§¸ ìƒí’ˆë¶€í„° ì‹œì‘

                        # ë¬´í•œ ëª¨ë“œ(product_count=None)ì´ë©´ ê³„ì† ìˆ˜ì§‘, ì•„ë‹ˆë©´ ê°œìˆ˜ ì œí•œ
                        while (self.product_count is None or found_count < self.product_count) and idx < len(all_product_elements):
                            # ì¤‘ì§€ ìš”ì²­ í™•ì¸
                            if self.should_stop:
                                print(f"[ì¤‘ì§€] ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ìˆ˜ì§‘ ì¤‘ì§€ ({found_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ)")
                                break

                            print(f"\n[{idx+1}ë²ˆì§¸ ìƒí’ˆ] ìˆ˜ì§‘ ì¤‘...")

                            # ë§¤ë²ˆ ìƒˆë¡œ element ì°¾ê¸° (DOM ë³€ê²½ ëŒ€ì‘) - ì´ë¯¸ì§€ ìˆëŠ” ë§í¬ë§Œ
                            current_elements = await page.query_selector_all('a[href*="/products/"]:has(img)')
                            if idx >= len(current_elements):
                                print(f"[ê²½ê³ ] ë” ì´ìƒ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
                                break

                            product_elem = current_elements[idx]
                            href = await product_elem.get_attribute('href')

                            try:
                                # ìƒí’ˆ í´ë¦­ (viewportë¡œ ìŠ¤í¬ë¡¤ í›„ í´ë¦­)
                                try:
                                    # 1. Elementë¥¼ í™”ë©´ì— ë³´ì´ë„ë¡ ìŠ¤í¬ë¡¤
                                    await product_elem.scroll_into_view_if_needed()
                                    await asyncio.sleep(0.5)

                                    # 2. í´ë¦­ ì‹œë„ (íƒ€ì„ì•„ì›ƒ 10ì´ˆ)
                                    await product_elem.click(timeout=10000)
                                except Exception as click_error:
                                    # 3. ì‹¤íŒ¨ ì‹œ ê°•ì œ í´ë¦­ ì‹œë„
                                    print(f"   [ì¬ì‹œë„] ê°•ì œ í´ë¦­ ì‹œë„...")
                                    await product_elem.click(force=True, timeout=5000)

                                await asyncio.sleep(3)

                                # ìƒˆ íƒ­ ì°¾ê¸°
                                all_pages = context.pages
                                if len(all_pages) <= 1:
                                    print(f"#{idx+1} [SKIP] íƒ­ ì—´ë¦¼ ì‹¤íŒ¨")
                                    idx += 1
                                    continue

                                detail_page = all_pages[-1]
                                await detail_page.wait_for_load_state('domcontentloaded')
                                await asyncio.sleep(1)

                                # URL ê²€ì¦: ìƒí’ˆ í˜ì´ì§€ì¸ì§€ í™•ì¸
                                current_url = detail_page.url
                                if not re.search(r'/products/\d+', current_url):
                                    print(f"#{idx+1} [SKIP] ì˜ëª»ëœ í˜ì´ì§€ (ìŠ¤í† ì–´ í˜ì´ì§€?): {current_url[:50]}")
                                    await detail_page.close()
                                    idx += 1
                                    continue

                                try:
                                    await detail_page.wait_for_load_state('networkidle', timeout=10000)
                                except:
                                    pass
                                await asyncio.sleep(1)

                                # ìŠ¤í¬ë¡¤ (ê²€ìƒ‰íƒœê·¸ ìœ„ì¹˜)
                                await detail_page.evaluate('window.scrollTo(0, document.body.scrollHeight * 0.4)')
                                await asyncio.sleep(2)

                                # ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
                                self.product_data = {}
                                self.product_data['product_url'] = href
                                await self._collect_detail_page_info(detail_page)

                                # í•œ ì¤„ ìš”ì•½ ì¶œë ¥
                                detail_info = self.product_data.get('detail_page_info', {})
                                product_name = detail_info.get('detail_product_name', 'N/A')[:30]
                                tags_count = len(detail_info.get('search_tags', []))

                                # ìˆ˜ì§‘ ì™„ë£Œ
                                self.products_data.append(self.product_data.copy())
                                found_count += 1

                                if self.product_count is None:
                                    print(f"#{idx+1} [{product_name}] - íƒœê·¸ {tags_count}ê°œ (ì´ {found_count}ê°œ)")
                                else:
                                    print(f"#{idx+1} [{product_name}] - íƒœê·¸ {tags_count}ê°œ ({found_count}/{self.product_count})")

                                # íƒ­ ë‹«ê¸°
                                await detail_page.close()
                                await asyncio.sleep(1)

                            except Exception as e:
                                print(f"#{idx+1} [ERROR] {str(e)[:50]}")
                                try:
                                    if len(context.pages) > 2:
                                        await context.pages[-1].close()
                                except:
                                    pass

                            idx += 1

                    print(f"\n{'='*60}")
                    print(f"[ì™„ë£Œ] ì´ {found_count}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ!")
                    print(f"[ì´ í™•ì¸] {idx}ê°œ ìƒí’ˆ í™•ì¸")
                    print(f"{'='*60}")

                # ë¸Œë¼ìš°ì € 30ì´ˆ ë” ì—´ì–´ë‘  (í™•ì¸ìš©)
                print("\n[ì™„ë£Œ] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
                print("â° ë¸Œë¼ìš°ì €ë¥¼ 30ì´ˆ í›„ ìë™ìœ¼ë¡œ ë‹«ìŠµë‹ˆë‹¤...")
                await asyncio.sleep(30)

                await browser.close()
                return self.products_data

            except Exception as e:
                print(f"[ì˜¤ë¥˜] {str(e)}")
                import traceback
                traceback.print_exc()
                return None

    async def _collect_product_info(self, page, product_elem):
        """ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘"""
        print("\n[ìˆ˜ì§‘] ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")

        info = {}

        # ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        container = await product_elem.evaluate_handle(
            'el => el.closest("li, div[class*=\'product\']")'
        )

        # ìƒí’ˆëª…
        title_elem = await container.query_selector('[class*="title"], [class*="name"]')
        if title_elem:
            info['product_name'] = await title_elem.inner_text()
            print(f"âœ“ ìƒí’ˆëª…: {info['product_name'][:40]}...")

        # ê°€ê²©
        price_elem = await container.query_selector('[class*="price"] strong')
        if price_elem:
            price_text = await price_elem.inner_text()
            info['price'] = price_text.replace(',', '').replace('ì›', '')
            print(f"âœ“ ê°€ê²©: {info['price']}ì›")

        # ë¸Œëœë“œ
        brand_elem = await container.query_selector('[class*="brand"], [class*="mall"]')
        if brand_elem:
            info['brand'] = await brand_elem.inner_text()
            print(f"âœ“ ë¸Œëœë“œ/ëª°: {info['brand']}")

        # ë¦¬ë·°
        review_elem = await container.query_selector('[class*="review"]')
        if review_elem:
            review_text = await review_elem.inner_text()
            # ë¦¬ë·° ìˆ˜ ì¶”ì¶œ
            review_match = re.search(r'ë¦¬ë·°\s*([0-9,]+)', review_text)
            if review_match:
                info['review_count'] = review_match.group(1).replace(',', '')
                print(f"âœ“ ë¦¬ë·° ìˆ˜: {info['review_count']}ê°œ")

            # í‰ì  ì¶”ì¶œ
            rating_match = re.search(r'(\d+\.\d+)', review_text)
            if rating_match:
                info['rating'] = rating_match.group(1)
                print(f"âœ“ í‰ì : {info['rating']}")

        # í• ì¸ìœ¨
        discount_elem = await container.query_selector('[class*="discount"]')
        if discount_elem:
            discount_text = await discount_elem.inner_text()
            discount_match = re.search(r'(\d+)%', discount_text)
            if discount_match:
                info['discount_rate'] = discount_match.group(1)
                print(f"âœ“ í• ì¸ìœ¨: {info['discount_rate']}%")

        # ë°°ì†¡ ì •ë³´
        delivery_elem = await container.query_selector('[class*="delivery"]')
        if delivery_elem:
            info['delivery'] = await delivery_elem.inner_text()
            print(f"âœ“ ë°°ì†¡: {info['delivery']}")

        # URL
        href = await product_elem.get_attribute('href')
        if href:
            info['url'] = href
            # ìƒí’ˆ ID ì¶”ì¶œ
            id_match = re.search(r'/products/(\d+)', href)
            if id_match:
                info['product_id'] = id_match.group(1)
                print(f"âœ“ ìƒí’ˆ ID: {info['product_id']}")

        # ì¸ë„¤ì¼
        img_elem = await container.query_selector('img')
        if img_elem:
            info['thumbnail_url'] = await img_elem.get_attribute('src')
            print(f"âœ“ ì¸ë„¤ì¼: ìˆ˜ì§‘ ì™„ë£Œ")

        self.product_data = {
            'category': self.category_name,
            'crawled_at': datetime.now().isoformat(),
            'product_info': info
        }

        print(f"\n[ì™„ë£Œ] ì´ {len(info)}ê°œ í•­ëª© ìˆ˜ì§‘")
        return info

    async def _check_search_tags(self, page):
        """ê²€ìƒ‰íƒœê·¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ê´€ë ¨ íƒœê·¸ ì„¹ì…˜ì˜ í•´ì‹œíƒœê·¸)"""
        try:
            print(f"   [ë””ë²„ê¹…] ê²€ìƒ‰íƒœê·¸ ì°¾ê¸° ì‹œì‘...")

            # ë°©ë²• 1: "ê´€ë ¨ íƒœê·¸" í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
            related_tag_section = await page.query_selector('text="ê´€ë ¨ íƒœê·¸"')
            if related_tag_section:
                print(f"   [ë””ë²„ê¹…] 'ê´€ë ¨ íƒœê·¸' ì„¹ì…˜ ë°œê²¬!")

            # ë°©ë²• 2: í˜ì´ì§€ ë‚´ ëª¨ë“  ë§í¬ì—ì„œ # ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í…ìŠ¤íŠ¸ ì°¾ê¸°
            all_links = await page.query_selector_all('a')
            print(f"   [ë””ë²„ê¹…] ì´ {len(all_links)}ê°œ ë§í¬ í™•ì¸ ì¤‘...")

            found_tags = []
            # ì œí•œ ì œê±°: ëª¨ë“  ë§í¬ í™•ì¸
            for idx, link in enumerate(all_links):
                try:
                    text = await link.inner_text()
                    if text and text.strip().startswith('#'):
                        clean_tag = text.strip()
                        if 2 < len(clean_tag) < 30 and clean_tag not in found_tags:
                            found_tags.append(clean_tag)
                            if len(found_tags) <= 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                                print(f"   â†’ ê²€ìƒ‰íƒœê·¸ ë°œê²¬: {clean_tag}")
                except:
                    continue

            if found_tags:
                print(f"   âœ“ ì´ {len(found_tags)}ê°œ ê²€ìƒ‰íƒœê·¸ ë°œê²¬!")
                return True

            # ë°©ë²• 3: URL íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸°
            tag_links = await page.query_selector_all('a[href*="/search"], a[href*="tag"], a[href*="%23"]')
            print(f"   [ë””ë²„ê¹…] URL íŒ¨í„´ìœ¼ë¡œ {len(tag_links)}ê°œ ë§í¬ ë°œê²¬")

            for link in tag_links[:20]:
                try:
                    text = await link.inner_text()
                    href = await link.get_attribute('href')
                    if text and text.strip().startswith('#'):
                        print(f"   â†’ ê²€ìƒ‰íƒœê·¸ ë°œê²¬: {text.strip()} (URL: {href[:50]}...)")
                        return True
                except:
                    continue

            print(f"   âœ— ê²€ìƒ‰íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
        except Exception as e:
            print(f"   [ì˜¤ë¥˜] íƒœê·¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

    async def _collect_detail_page_info(self, page):
        """
        ìƒì„¸ í˜ì´ì§€ì—ì„œ DB ìŠ¤í‚¤ë§ˆì— ë§ì¶° ëª¨ë“  ì •ë³´ ìˆ˜ì§‘
        ë¦¬íŒ©í† ë§: config ê¸°ë°˜ ë‹¤ì¤‘ fallback ì‹œìŠ¤í…œ ì‚¬ìš©
        """
        detail_info = {}

        # 1. ìƒí’ˆëª… (product_name) - TEXT NOT NULL
        elem = await self.helper.try_selectors(page, SELECTORS['product_name'], "ìƒí’ˆëª…")
        detail_info['detail_product_name'] = await self.helper.extract_text(elem, "ìƒí’ˆëª…")

        # 2. ë¸Œëœë“œëª… (brand_name) - VARCHAR(100)
        elem = await self.helper.try_selectors(page, SELECTORS['brand_name'], "ë¸Œëœë“œ")
        detail_info['brand_name'] = await self.helper.extract_text(elem, "ë¸Œëœë“œ")

        # 3. ê°€ê²© (price) - INTEGER
        elem = await self.helper.try_selectors(page, SELECTORS['price'], "ê°€ê²©")
        price_text = await self.helper.extract_text(elem, "ê°€ê²©")
        detail_info['detail_price'] = self.helper.clean_price(price_text)

        # 4. í• ì¸ìœ¨ (discount_rate) - INTEGER
        elem = await self.helper.try_selectors(page, SELECTORS['discount_rate'], "í• ì¸ìœ¨")
        discount_text = await self.helper.extract_text(elem, "í• ì¸ìœ¨")
        detail_info['discount_rate'] = self.helper.clean_discount_rate(discount_text)

        # 5. ë¦¬ë·° ìˆ˜ (review_count) - INTEGER
        elem = await self.helper.try_selectors(page, SELECTORS['review_count'], "ë¦¬ë·° ìˆ˜")
        review_text = await self.helper.extract_text(elem, "ë¦¬ë·° ìˆ˜")
        detail_info['detail_review_count'] = self.helper.clean_review_count(review_text)

        # 6. í‰ì  (rating) - DECIMAL(2,1)
        elem = await self.helper.try_selectors(page, SELECTORS['rating'], "í‰ì ")
        rating_text = await self.helper.extract_text(elem, "í‰ì ")
        detail_info['rating'] = self.helper.clean_rating(rating_text)

        # 7. ê²€ìƒ‰ íƒœê·¸ (search_tags) - TEXT[]
        # êµ¬ì¡° ê¸°ë°˜: "ê´€ë ¨ íƒœê·¸" ì°¾ì€ í›„ ë‹¤ìŒ ul > a ë¦¬ìŠ¤íŠ¸
        tags = []
        try:
            # ë¨¼ì € "ê´€ë ¨ íƒœê·¸" í…ìŠ¤íŠ¸ ì°¾ê¸°
            tag_container = await self.helper.find_by_text_then_next(
                page, "ê´€ë ¨ íƒœê·¸", "ul", "ê²€ìƒ‰íƒœê·¸ ì»¨í…Œì´ë„ˆ"
            )

            if tag_container:
                # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ë§í¬ ì°¾ê¸°
                tag_links = await self.helper.try_selectors_from_element(
                    tag_container, SELECTORS['search_tags_links'], "ê²€ìƒ‰íƒœê·¸ ë§í¬", multiple=True
                )

                if tag_links:
                    for link in tag_links:
                        text = await self.helper.extract_text(link, "íƒœê·¸")
                        if text:
                            # # ì œê±°í•˜ê³  ì €ì¥
                            clean_tag = text.replace('#', '').strip()
                            if 1 < len(clean_tag) < 30 and clean_tag not in tags:
                                tags.append(clean_tag)
            else:
                # Fallback: ì „ì²´ í˜ì´ì§€ì—ì„œ # ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë§í¬ ì°¾ê¸°
                all_links = await page.query_selector_all('a')
                for link in all_links[:100]:  # ìµœëŒ€ 100ê°œë§Œ í™•ì¸
                    text = await self.helper.extract_text(link)
                    if text and text.strip().startswith('#'):
                        clean_tag = text.strip().replace('#', '').strip()
                        if 1 < len(clean_tag) < 30 and clean_tag not in tags:
                            tags.append(clean_tag)

            if tags:
                detail_info['search_tags'] = tags
                print(f"   [ê²€ìƒ‰íƒœê·¸] {len(tags)}ê°œ ìˆ˜ì§‘")
            else:
                detail_info['search_tags'] = []
                print(f"   [ê²€ìƒ‰íƒœê·¸] ì—†ìŒ")

        except Exception as e:
            print(f"   [ê²€ìƒ‰íƒœê·¸ ì˜¤ë¥˜] {str(e)[:50]}")
            detail_info['search_tags'] = []

        # 8. ì¸ë„¤ì¼ (thumbnail_url) - TEXT
        elem = await self.helper.try_selectors(page, SELECTORS['thumbnail'], "ì¸ë„¤ì¼")
        detail_info['thumbnail_url'] = await self.helper.extract_attribute(elem, "src", "ì¸ë„¤ì¼")

        # 9. í’ˆì ˆ ì—¬ë¶€ (is_sold_out) - BOOLEAN
        elem = await self.helper.try_selectors(page, SELECTORS['is_sold_out'], "í’ˆì ˆ")
        detail_info['is_sold_out'] = (elem is not None)

        # 10. URL (product_url) - TEXT
        detail_info['detail_page_url'] = page.url

        # 11. ì¹´í…Œê³ ë¦¬ ì „ì²´ ê²½ë¡œ (category_fullname) - VARCHAR(500)
        # ì˜ˆ: "ì–¸ë”ì›¨ì–´>ì—¬ì„±>ë¸Œë¼"
        category_path = []
        try:
            # 1ìˆœìœ„: êµ¬ì¡° ê¸°ë°˜ (ê°€ì¥ ì•ˆì •ì )
            breadcrumb_links = await self.helper.find_breadcrumb_from_home(page, "ì¹´í…Œê³ ë¦¬ ê²½ë¡œ")

            # 2ìˆœìœ„: config ê¸°ë°˜ fallback
            if not breadcrumb_links:
                breadcrumb_links = await self.helper.try_selectors(
                    page, SELECTORS['category_breadcrumb'], "ì¹´í…Œê³ ë¦¬ ê²½ë¡œ", multiple=True
                )

            if breadcrumb_links:
                for link in breadcrumb_links:
                    text = await self.helper.extract_text(link)
                    if text:
                        # "í™ˆ", "ì „ì²´", "ì‡¼í•‘í™ˆ" ê°™ì€ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
                        clean_text = text.strip()
                        if clean_text and clean_text not in ['í™ˆ', 'ì „ì²´', 'ì‡¼í•‘í™ˆ', 'ì‡¼í•‘', 'HOME']:
                            category_path.append(clean_text)

                if category_path:
                    detail_info['category_fullname'] = '>'.join(category_path)
                    print(f"   [ì¹´í…Œê³ ë¦¬ ê²½ë¡œ] {detail_info['category_fullname']}")
                else:
                    detail_info['category_fullname'] = self.category_name  # fallback to main category
                    print(f"   [ì¹´í…Œê³ ë¦¬ ê²½ë¡œ] breadcrumb í…ìŠ¤íŠ¸ ì—†ìŒ, ëŒ€ë¶„ë¥˜ ì‚¬ìš©: {self.category_name}")
            else:
                detail_info['category_fullname'] = self.category_name  # fallback
                print(f"   [ì¹´í…Œê³ ë¦¬ ê²½ë¡œ] ì°¾ì„ ìˆ˜ ì—†ìŒ, ëŒ€ë¶„ë¥˜ ì‚¬ìš©: {self.category_name}")

        except Exception as e:
            print(f"   [ì¹´í…Œê³ ë¦¬ ê²½ë¡œ ì˜¤ë¥˜] {str(e)[:50]}")
            detail_info['category_fullname'] = self.category_name  # fallback

        # ë°ì´í„° ì €ì¥
        if self.product_data:
            self.product_data['detail_page_info'] = detail_info
        else:
            self.product_data = {
                'category': self.category_name,
                'crawled_at': datetime.now().isoformat(),
                'detail_page_info': detail_info
            }

        return detail_info

    def save_to_json(self):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not self.products_data:
            print("[ê²½ê³ ] ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'data/womens_products_{timestamp}.json'

        # ì—¬ëŸ¬ ìƒí’ˆ ë°ì´í„°ë¥¼ ë°°ì—´ë¡œ ì €ì¥
        output = {
            'category': self.category_name,
            'total_count': len(self.products_data),
            'crawled_at': timestamp,
            'products': self.products_data
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"\n[ì €ì¥] {filename}")
        print(f"[ì €ì¥] ì´ {len(self.products_data)}ê°œ ìƒí’ˆ ì €ì¥ë¨")
        return filename

    def save_to_db(self, skip_duplicates=True):
        """DBì— ì €ì¥ (ì¤‘ë³µ ì²´í¬ ì„ íƒ ê°€ëŠ¥)"""
        if not self.products_data:
            print("[ê²½ê³ ] DBì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        print("\n[DB ì €ì¥] PostgreSQLì— ì €ì¥ ì‹œì‘...")
        print(f"ì´ {len(self.products_data)}ê°œ ìƒí’ˆì„ DBì— ì €ì¥í•©ë‹ˆë‹¤:")
        print(f"ì¤‘ë³µ ì²´í¬: {'í™œì„±í™”' if skip_duplicates else 'ë¹„í™œì„±í™”'}\n")

        try:
            # DB ì €ì¥ ì‹¤í–‰
            results = save_to_database(self.category_name, self.products_data, skip_duplicates=skip_duplicates)

            print(f"\nâœ… DB ì €ì¥ ì™„ë£Œ:")
            print(f"   - ì‹ ê·œ ì €ì¥: {results['saved']}ê°œ")
            print(f"   - ì¤‘ë³µ ìŠ¤í‚µ: {results['skipped']}ê°œ")
            print(f"   - ì €ì¥ ì‹¤íŒ¨: {results['failed']}ê°œ")

            return results['saved'] > 0 or results['skipped'] > 0

        except Exception as e:
            print(f"\nâŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return False

    def print_summary(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*60)
        print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("="*60)

        if self.products_data:
            print(f"\nì´ ìˆ˜ì§‘ ìƒí’ˆ: {len(self.products_data)}ê°œ")
            print("\n[ìˆ˜ì§‘ëœ ìƒí’ˆ ëª©ë¡]")

            for idx, product in enumerate(self.products_data, 1):
                info = product.get('product_info', {})
                detail = product.get('detail_page_info', {})

                print(f"\n{idx}. {info.get('product_name', 'N/A')[:50]}")
                print(f"   - ID: {info.get('product_id', 'N/A')}")
                print(f"   - ë¦¬ë·°: {info.get('review_count', 'N/A')}ê°œ")
                print(f"   - í‰ì : {info.get('rating', 'N/A')}")
                if detail:
                    print(f"   - ìƒì„¸ì •ë³´: âœ… ìˆ˜ì§‘ì™„ë£Œ")
        else:
            print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        print("="*60)


if __name__ == "__main__":
    async def main():
        print("\n" + "="*60)
        print("ì—¬ì„±ì˜ë¥˜ ìƒí’ˆ í¬ë¡¤ë§ - ì „ì²´ ìƒí’ˆ ìˆ˜ì§‘ (ì¤‘ë³µ ì²´í¬)")
        print("="*60)

        # 20ê°œ ìƒí’ˆ ìˆ˜ì§‘
        crawler = WomensClothingManualCaptcha(product_count=20, enable_screenshot=False)

        # í¬ë¡¤ë§ ì‹¤í–‰
        data = await crawler.crawl_with_manual_captcha()

        if crawler.products_data:
            # JSON ì €ì¥
            crawler.save_to_json()

            # DB ì €ì¥ (ì¤‘ë³µ ì²´í¬)
            crawler.save_to_db()

            # ìš”ì•½ ì¶œë ¥
            crawler.print_summary()
        else:
            print("\n[ì‹¤íŒ¨] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

    asyncio.run(main())